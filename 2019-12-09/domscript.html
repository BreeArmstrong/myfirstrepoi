<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Title</title>
  <style type="text/css">
    body {
      min-height: 100vh;
      background-color: #eee;
      margin: 0;
      --sans-serif: -apple-system, BlinkMacSystemFont, "avenir next", avenir, helvetica, "helvetica neue", ubuntu, roboto, noto, "segoe ui", arial, sans-serif;
      font-family: -apple-system, BlinkMacSystemFont, "avenir next", avenir, helvetica, "helvetica neue", ubuntu, roboto, noto, "segoe ui", arial, sans-serif;;
    }

    .circle {
      width: 3em;
      height: 3em;
      background-color: red;
      border-radius: 100%; /*Read up from MDN for border radius*/

      position: fixed;
      transition: all 100ms ease;
    }

    main {
      padding-top: 20em;
      position: relative;
    }
  </style>
  <script src="domscript.js" type="application/javascript"></script>
</head>
<body>
<main>
  <div>
    <ul>
      <li>Static</li>
      <li>
        Relative
        <ul>
          <li>It preserves the whitespace of the element</li>
          <li>It positions (top, left) <strong>based on it's original position</strong></li>
        </ul>
      </li>
      <li>Absolute
        <ul>
          <li>It does not preserve the whitespace of the element.</li>
          <li>It positions itself with respect to the nearest <strong>non-static ancestor</strong></li>
        </ul>
      </li>
      <li>Fixed
        <ul>
          <li>It does not preserve the whitespace of the element.</li>
          <li>It positions itself with respect to the <strong>viewport</strong></li>
        </ul>
      </li>
    </ul>
  </div>

  Overview
  The name machine learning was coined in 1959 by Arthur Samuel.[5] Tom M. Mitchell provided a widely quoted, more
  formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from
  experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as
  measured by P, improves with experience E."[6] This definition of the tasks in which machine learning is concerned
  offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan
  Turing's proposal in his paper "Computing Machinery and Intelligence", in which the question "Can machines think?"
  is replaced with the question "Can machines do what we (as thinking entities) can do?".[7] In Turing's proposal the
  various characteristics that could be possessed by a thinking machine and the various implications in constructing
  one are exposed.

  Machine learning tasks


  A support vector machine is a supervised learning model that divides the data into regions separated by a linear
  boundary. Here, the linear boundary divides the black circles from the white.
  Machine learning tasks are classified into several broad categories. In supervised learning, the algorithm builds a
  mathematical model from a set of data that contains both the inputs and the desired outputs. For example, if the
  task were determining whether an image contained a certain object, the training data for a supervised learning
  algorithm would include images with and without that object (the input), and each image would have a label (the
  output) designating whether it contained the object. In special cases, the input may be only partially available, or
  restricted to special feedback.[clarification needed] Semi-supervised learning algorithms develop mathematical
  models from incomplete training data, where a portion of the sample input doesn't have labels.

  Classification algorithms and regression algorithms are types of supervised learning. Classification algorithms are
  used when the outputs are restricted to a limited set of values. For a classification algorithm that filters emails,
  the input would be an incoming email, and the output would be the name of the folder in which to file the email. For
  an algorithm that identifies spam emails, the output would be the prediction of either "spam" or "not spam",
  represented by the Boolean values true and false. Regression algorithms are named for their continuous outputs,
  meaning they may have any value within a range. Examples of a continuous value are the temperature, length, or price
  of an object.

  In unsupervised learning, the algorithm builds a mathematical model from a set of data which contains only inputs
  and no desired output labels. Unsupervised learning algorithms are used to find structure in the data, like grouping
  or clustering of data points. Unsupervised learning can discover patterns in the data, and can group the inputs into
  categories, as in feature learning. Dimensionality reduction is the process of reducing the number of "features", or
  inputs, in a set of data.

  Active learning algorithms access the desired outputs (training labels) for a limited set of inputs based on a
  budget, and optimize the choice of inputs for which it will acquire training labels. When used interactively, these
  can be presented to a human user for labeling. Reinforcement learning algorithms are given feedback in the form of
  positive or negative reinforcement in a dynamic environment, and are used in autonomous vehicles or in learning to
  play a game against a human opponent.[2]:3 Other specialized algorithms in machine learning include topic modeling,
  where the computer program is given a set of natural language documents and finds other documents that cover similar
  topics. Machine learning algorithms can be used to find the unobservable probability density function in density
  estimation problems. Meta learning algorithms learn their own inductive bias based on previous experience. In
  developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known
  as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with
  humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies, and
  imitation.[clarification needed]

  History and relationships to other fields
  See also: Timeline of machine learning
  Arthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term
  "Machine Learning" in 1959 while at IBM.[8] A representative book of the machine learning research during 1960s was
  the Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[9] The
  interest of machine learning related to pattern recognition continued during 1970s, as described in the book of Duda
  and Hart in 1973. [10] In 1981 a report was given on using teaching strategies so that a neural network learns to
  recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. [11] As a
  scientific endeavor, machine learning grew out of the quest for artificial intelligence. Already in the early days
  of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted
  to approach the problem with various symbolic methods, as well as what were then termed "neural networks"; these
  were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models
  of statistics.[12] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[13]:488

  However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine
  learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and
  representation.[13]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[14] Work
  on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more
  statistical line of research was now outside the field of AI proper, in pattern recognition and information
  retrieval.[13]:708â€“710; 755 Neural networks research had been abandoned by AI and computer science around the same
  time. This line, too, was continued outside the AI/CS field, as "connectionism", by researchers from other
  disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention
  of backpropagation.[13]:25

  Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from
  achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from
  the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and
  probability theory.[14] It also benefited from the increasing availability of digitized information, and the ability
  to distribute it via the Internet.

  Relation to data mining
  Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning
  focuses on prediction, based on known properties learned from the training data, data mining focuses on the
  discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in
  databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine
  learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner
  accuracy. Much of the confusion between these two research communities (which do often have separate conferences and
  separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine
  learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in
  knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated
  with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised
  methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.

  Relation to optimization
  Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of
  some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of
  the model being trained and the actual problem instances (for example, in classification, one wants to assign a
  label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The
  difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize
  the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[15]

  Relation to statistics
  Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal
  goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive
  patterns.[16] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to
  theoretical tools, have had a long pre-history in statistics.[17] He also suggested the term data science as a
  placeholder to call the overall field.[17]

  Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[18] wherein
  "algorithmic model" means more or less the machine learning algorithms like Random forest.

  Some statisticians have adopted methods from machine learning, leading to a combined field that they call
  statistical learning.[19]
</main>
</body>
</html>